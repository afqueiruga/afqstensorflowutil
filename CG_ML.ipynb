{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjugate Gradients on Models\n",
    "\n",
    "Suppose we have some cost function we want to optimize to determine parameters $w$:\n",
    "\\begin{equation}\n",
    "\\min_w g(w)\n",
    "\\end{equation}\n",
    "This is the same problem as \n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial w} g(w) = 0\n",
    "\\end{equation}\n",
    "If $g$ is the loss function of a neural network or anything with a machine-learn-y name, people are just going to use stochastic gradient descent, which is naive but works well.\n",
    "\n",
    "If the model is a linear system of equations, then its equivalent cost function is\n",
    "\\begin{equation}\n",
    "g(w) = \\frac{1}{2} w^T\\mathbf{A}w-w^T\\mathbf{b}\n",
    "\\end{equation}\n",
    "The loss function's gradient is the system of equation $\\mathbf{A}w-\\mathbf{b}=0$, and one-step of Newton's method would involve just solving that same equation.\n",
    "\n",
    "## The method\n",
    "\n",
    "\n",
    "- $r := \\frac{\\partial}{\\partial w} g(w_k)$\n",
    "- $z := r$\n",
    "- Do until convergence:\n",
    "  - $\\alpha := r\\cdot r \\,/\\, z\\cdot H z$\n",
    "  - $w += \\alpha z$\n",
    "  - $r_{new} := \\frac{\\partial}{\\partial w} g(w_k)$\n",
    "  - $\\beta = r_{new} \\cdot r_{new}\\,/\\,r_{old}\\cdot r_{old}$\n",
    "  - $ z_{new} = r_{new} + \\beta z$\n",
    "  \n",
    "## The Action of the Hessian\n",
    "\n",
    "We can compute the Hessian for arbitrary models (look in [operations.py](operations.py)`:NewtonsMethod()`), but this suffers from two problems:\n",
    "\n",
    "- it will be huge and dense, $N_{param}\\times N_{param}$\n",
    "- generating the graph can blow up.\n",
    "\n",
    "Just dealing the Hessian is one issue with naively doing more advanced methods. Then, for arbitrary models with unneccessary parameters, the Hessian itself is likely rank defficient and cannot be used for the direct application of Newton's method. However, it turns out that we don't need the Hessian, what we need is its linear action on an arbitrary vector:\n",
    "\\begin{equation}\n",
    "H \\mathbf{z}\n",
    "\\end{equation}\n",
    "\n",
    "We can avoid assembling a large dense matrix by remembering the interchangability of derivatives:\n",
    "\\begin{equation}\n",
    "\\left[\\frac{\\partial^2 g}{\\partial w \\partial w}\\right] \\mathbf{z} = \\frac{\\partial}{\\partial x} \\left(\\frac{\\partial g}{\\partial x} \\cdot \\mathbf{z} \\right)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing it!\n",
    "\n",
    "Let's do a proof of concept with a tiny linear fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from operations import CatVariable, flatpack\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.Variable(tf.truncated_normal(shape=(1,1)))\n",
    "b = tf.Variable(tf.truncated_normal(shape=(1,)))\n",
    "x = tf.constant([0,1],dtype=tf.float32)\n",
    "y_true = tf.constant([[2,5]],dtype=tf.float32)\n",
    "y_pred = a*x+b\n",
    "goal = tf.losses.mean_squared_error( y_true,y_pred)\n",
    "params = [a, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvmul = lambda K,x : tf.einsum('ij,i->j',K,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = lambda a,b : tf.reduce_sum([ tf.tensordot(p_,z_, (range(len(p_.shape)),range(len(z_.shape))) )\n",
    "                                   for p_,z_ in zip(a,b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.truncated_normal(shape=(2,)))\n",
    "A = tf.constant([[1.0,2.0],[2.0,3.0]])\n",
    "b = tf.constant([3.0,3.0]) # hashtag life-hack to get the answer\n",
    "# goal = tf.einsum('i,i->',w,0.5*A*w-b)\n",
    "goal = tf.tensordot(w,0.5*mvmul(A,w)-b, axes=1)\n",
    "params = [ w ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steepest descent is just this operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "steep_step = [ tf.assign(w_, w_-1.0e-3*g_) for w_,g_ in zip(params,grads) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No depdendencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_params = [tf.Variable(p_) for p_ in params] # The conjugates\n",
    "r_params = [tf.Variable(p_) for p_ in params] # A temp to store gradients\n",
    "alpha = tf.Variable(1.0)\n",
    "beta = tf.Variable(1.0)\n",
    "r_dot_r_new = tf.Variable(1.0)\n",
    "r_dot_r_old = tf.Variable(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tf.gradients(goal, params)\n",
    "z_dot_grad = dot(z_params,grads)\n",
    "Hz = tf.gradients(z_dot_grad,params)\n",
    "r_dot_r = dot(r_params,r_params)\n",
    "z_dot_Hz = dot(z_params,Hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to make sure we can extract out the values of A we expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1.], dtype=float32), [array([2., 3.], dtype=float32)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([tf.assign(z_params[0],[0.0,1.0]),Hz])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_z    = [ tf.assign(z_,           -g_) for z_,g_ in zip(z_params,grads) ]\n",
    "calc_grad  = [ tf.assign(r_,           -g_) for r_,g_ in zip(r_params,grads) ]\n",
    "calc_r_dot_r=[ tf.assign( r_dot_r_old, r_dot_r ) ]\n",
    "calc_alpha = [ tf.assign(alpha,        r_dot_r_old/z_dot_Hz) ]\n",
    "calc_newp  = [ tf.assign(p_,           p_+alpha*z_) for p_,z_ in zip(params,z_params)  ]\n",
    "calc_beta  = [ tf.assign(beta,         r_dot_r/r_dot_r_old) ]\n",
    "calc_z     = [ tf.assign(z_,           r_+beta*z_) for z_,r_ in zip(z_params,r_params) ]\n",
    "\n",
    "cg_first_no_flow = reset_z + calc_grad\n",
    "cg_step_no_flow = calc_r_dot_r + calc_alpha + calc_newp + calc_grad + calc_beta + calc_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because that program listing has assignments, we have to do the sess.runs() one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.9980001, 2.9970002], dtype=float32), array([1.9980001, 2.9970002], dtype=float32)]\n",
      "[1.4970016, [array([-3.   ,  2.001], dtype=float32)], [array([ 7.9920006, 12.987001 ], dtype=float32)]]\n",
      "12.974014\n",
      "0.23636362\n",
      "[-2.5277455  2.7093818]\n",
      "[ 0.10898185 -0.07265425]\n",
      "0.0013223111\n",
      "[ 0.11162382 -0.06869128]\n",
      "[-0.036291122, 0.23636362, 0.0013223111, [array([-2.5277455,  2.7093818], dtype=float32)], [array([-0.10898185,  0.07265425], dtype=float32)]]\n",
      "0.017155683\n",
      "-4.230769\n",
      "[-3.         2.9999988]\n",
      "[2.3841858e-06 3.8146973e-06]\n",
      "1.1795658e-09\n",
      "[2.3843174e-06 3.8146163e-06]\n",
      "[0.0, -4.230769, 1.1795658e-09, [array([-3.       ,  2.9999988], dtype=float32)], [array([-2.3841858e-06, -3.8146973e-06], dtype=float32)]]\n",
      "2.0236257e-11\n",
      "0.23607424\n",
      "[-2.9999995  2.9999998]\n",
      "[-0. -0.]\n",
      "0.0\n",
      "[0. 0.]\n",
      "[-4.7683716e-07, 0.23607424, 0.0, [array([-2.9999995,  2.9999998], dtype=float32)], [array([0., 0.], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.assign(w,[-3.00,2.001]))\n",
    "print(sess.run(cg_first_no_flow))\n",
    "print(sess.run([goal,params,Hz]))\n",
    "for i in range(3):\n",
    "    for s in cg_step_no_flow:\n",
    "        print(sess.run(s))\n",
    "    print(sess.run([goal,alpha,beta,params,grads]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With control flow:\n",
    "\n",
    "Now, when we include control flow to try to get *one operation* that tensorflow can execute, it turns into a complete mess:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tf.gradients(goal, params)\n",
    "reset_z            = [ tf.assign(z_,           -g_) for z_,g_ in zip(z_params,grads) ]\n",
    "calc_grad_initial  = [ tf.assign(r_,           -g_) for r_,g_ in zip(r_params,grads) ]\n",
    "z_dot_grad = dot(z_params,grads)\n",
    "Hz = tf.gradients(z_dot_grad,params)\n",
    "z_dot_Hz = dot(z_params,Hz)\n",
    "r_dot_r = dot(r_params,r_params)\n",
    "calc_r_dot_r=[ tf.assign( r_dot_r_old, r_dot_r ) ]\n",
    "with tf.control_dependencies(calc_r_dot_r):\n",
    "    calc_alpha = [ tf.assign(alpha,        r_dot_r_old/z_dot_Hz) ]\n",
    "    with tf.control_dependencies(calc_alpha):\n",
    "        calc_newp  = [ tf.assign(p_,           p_ + alpha*z_) for p_,z_ in zip(params,z_params)  ]\n",
    "        with tf.control_dependencies(calc_newp):\n",
    "            goal2 = tf.tensordot(w,0.5*mvmul(A,w)-b, axes=1)\n",
    "            grads = tf.gradients(goal2, params)\n",
    "\n",
    "            calc_grad_during  = [ tf.assign(r_,    -g_) for r_,g_ in zip(r_params,grads) ]\n",
    "            with tf.control_dependencies(calc_grad_during):\n",
    "                r_dot_r = dot(r_params,r_params)\n",
    "                calc_beta  = [ tf.assign(beta,         r_dot_r/r_dot_r_old) ]\n",
    "                with tf.control_dependencies(calc_beta):\n",
    "                    calc_z = [ tf.assign(z_,           r_ + beta*z_) for z_,r_ in zip(z_params,r_params) ]\n",
    "cg_first = reset_z + calc_grad_initial\n",
    "cg_step = calc_r_dot_r + calc_alpha + calc_newp + calc_grad_during + calc_beta + calc_z\n",
    "cg_step_1 = calc_r_dot_r + calc_alpha + calc_newp\n",
    "cg_step_2 = calc_grad_during + calc_beta + calc_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0.], dtype=float32), 1.0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([tf.assign(z_params[0],[1.0,0.0]),z_dot_Hz])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to run it the same way, but for some reason including the control flow directives **break everything**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1.9980001, 2.9970002], dtype=float32), array([1.9980001, 2.9970002], dtype=float32)]\n",
      "[1.4970016, [array([-3.   ,  2.001], dtype=float32)], [array([ 7.9920006, 12.987001 ], dtype=float32)]]\n",
      "12.974014\n",
      "0.23636362\n",
      "[-2.5277455  2.7093818]\n",
      "[-1.7800364 -3.1423092]\n",
      "4.0119076\n",
      "[6.7981625 8.535282 ]\n",
      "[55.87135, 0.95380926, 9.056986, [array([ 6.808075, 15.127918], dtype=float32)], [array([34.06391 , 55.999905], dtype=float32)]]\n",
      "473.91452\n",
      "0.95380926\n",
      "[13.292225 23.26895 ]\n",
      "[ -79.59634 -130.78268]\n",
      "229.29004\n",
      "[ 55288.406 -30936.125]\n",
      "[29339468000.0, -272.27548, 46288.555, [array([-14979802.,   8515906.], dtype=float32)], [array([ 2052007., -4411889.], dtype=float32)]]\n",
      "248777240000.0\n",
      "-272.27548\n",
      "[-30033480.  16939054.]\n",
      "[-5637245. 14087707.]\n",
      "99656.84\n",
      "[3.8029670e+14 1.4090865e+14]\n",
      "[-2.8811114e+29, 0.54103243, 9868888000.0, [array([-1.1826901e+15,  8.5312680e+14], dtype=float32)], [array([5.2356349e+14, 1.9400018e+14], dtype=float32)]]\n",
      "2.2644356e+29\n",
      "0.54103243\n",
      "[-9.7693729e+14  9.2936294e+14]\n",
      "[-1.2400137e+15 -1.4744284e+15]\n",
      "52.21711\n",
      "[ 4.9456555e+17 -2.6042816e+17]\n",
      "[4.6927945e+34, -6.135918, 2125.376, [array([-2.8559136e+18,  1.6654697e+18], dtype=float32)], [array([ 4.7502586e+17, -7.1541813e+17], dtype=float32)]]\n",
      "4.119141e+35\n",
      "-6.135918\n",
      "[-5.8905274e+18  3.2634357e+18]\n",
      "[-7.9766160e+17  3.2660784e+18]\n",
      "inf\n",
      "[nan nan]\n",
      "[nan, nan, nan, [array([nan, nan], dtype=float32)], [array([nan, nan], dtype=float32)]]\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.assign(w,[-3.00,2.001]))\n",
    "print(sess.run(cg_first))\n",
    "print(sess.run([goal,params,Hz]))\n",
    "for i in range(5):\n",
    "    for s in cg_step:\n",
    "        print(sess.run(s))\n",
    "    print(sess.run([goal,alpha,beta,params,grads]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After six hours of trying to implement a very simple program, I give up. TensorFlow is a great language for describing model architectures, but a horrendously poor languagefor describing algorithms. We shouldn't try to implement anything more complicated than gradient methods in TF. Maybe tf2 will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nA,nb=sess.run([A,b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.,  3.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.solve(nA,nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 3.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nA.dot([-3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
